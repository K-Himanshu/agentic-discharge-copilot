{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa070417",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1Ô∏è‚É£ INSTALL DEPENDENCIES\n",
    "# ===============================\n",
    "!pip install -q fastapi uvicorn pyngrok transformers accelerate torch sentencepiece nest_asyncio\n",
    "\n",
    "# ===============================\n",
    "# 2Ô∏è‚É£ IMPORTS\n",
    "# ===============================\n",
    "import torch\n",
    "import nest_asyncio\n",
    "import threading\n",
    "from fastapi import FastAPI\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from google.colab import userdata\n",
    "from pyngrok import ngrok\n",
    "from pyngrok import conf\n",
    "\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "conf.get_default().auth_token = userdata.get(\"ngrok_auth\")\n",
    "# ===============================\n",
    "# 3Ô∏è‚É£ LOAD MEDGEMMA (GPU)\n",
    "# ===============================\n",
    "MODEL_ID = \"google/medgemma-1.5-4b-it\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading model (this takes 1-2 mins)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "print(\"Model loaded successfully ‚úÖ\")\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "\n",
    "# ===============================\n",
    "# 4Ô∏è‚É£ BUILD FASTAPI APP\n",
    "# ===============================\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def health():\n",
    "    return {\"status\": \"MedGemma server running\"}\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "def generate(note: str):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a senior clinical discharge copilot.\\n\"\n",
    "                \"Return ONLY valid JSON.\\n\"\n",
    "                \"Do NOT include explanations.\\n\"\n",
    "                \"Follow this schema:\\n\"\n",
    "                \"{\"\n",
    "                '\"triage_level\": \"low|medium|high\",'\n",
    "                '\"medications\": [],'\n",
    "                '\"activity_guidance\": [],'\n",
    "                '\"warning_signs\": [],'\n",
    "                '\"red_flag_actions\": [],'\n",
    "                '\"follow_up\": [],'\n",
    "                '\"patient_instructions_simple\": []'\n",
    "                \"}\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Convert this discharge note:\\n\\n{note}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=400,     # ‚Üê increase to avoid truncation\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    generated_tokens = outputs[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "    response = tokenizer.batch_decode(\n",
    "        generated_tokens,\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "\n",
    "    response = response.strip()\n",
    "\n",
    "    # Remove markdown fences if present\n",
    "    response = response.replace(\"```json\", \"\")\n",
    "    response = response.replace(\"```\", \"\")\n",
    "\n",
    "    # Extract JSON block safely\n",
    "    start = response.find(\"{\")\n",
    "    end = response.rfind(\"}\")\n",
    "\n",
    "    if start != -1 and end != -1:\n",
    "        response = response[start:end+1]\n",
    "    else:\n",
    "        response = \"{}\"\n",
    "\n",
    "    return {\"response\": response}\n",
    "\n",
    "# ===============================\n",
    "# 5Ô∏è‚É£ START SERVER + NGROK\n",
    "# ===============================\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"üî• PUBLIC URL:\", public_url)\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "threading.Thread(target=run_server).start()\n",
    "\n",
    "print(\"Server is running...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
